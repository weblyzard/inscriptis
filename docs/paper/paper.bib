
@article{wang_review_2020,
	title = {A review of emotion sensing: categorization models and algorithms},
	issn = {1573-7721},
	shorttitle = {A review of emotion sensing},
	url = {https://doi.org/10.1007/s11042-019-08328-z},
	doi = {10.1007/s11042-019-08328-z},
	abstract = {Sentiment analysis consists in the identification of the sentiment polarity associated with a target object, such as a book, a movie or a phone. Sentiments reflect feelings and attitudes, while emotions provide a finer characterization of the sentiments involved. With the huge number of comments generated daily on the Internet, besides sentiment analysis, emotion identification has drawn keen interest from different researchers, businessmen and politicians for polling public opinions and attitudes. This paper reviews and discusses existing emotion categorization models for emotion analysis and proposes methods that enhance existing emotion research. We carried out emotion analysis by inviting experts from different research areas to produce comprehensive results. Moreover, a computational emotion sensing model is proposed, and future improvements are discussed in this paper.},
	language = {en},
	urldate = {2020-01-07},
	journal = {Multimedia Tools and Applications},
	author = {Wang, Zhaoxia and Ho, Seng-Beng and Cambria, Erik},
	month = jan,
	year = {2020},
	keywords = {Affective computing, Emotion categorization model, Emotion definition, Sentiment analysis},
	annote = {Summary
This paper discusses different emotion models such as Ekman's emotion model, Plutchik's wheel of emotions and the Hourglass of Emotions.

the models are based on different assumptions and models that may be inspired by psychology or biology.
There is a clear distinction between feeling (primarily determined by the person's internal state) and emotion (triggered by the environment).
The discussed models cover a total of 65 different emotions.
there are emotional categories such as surprise which might not have a polarity or for which the polarity might depend on the context. This has also been confirmed by an experiment in which experts evaluated the polarity of 65 emotions.
OCC emotion model - calibrated by considering the author's personality based on openness, conscientiousness, agreeableness, extraversion and neuroticism)

Datasets

The paper contains background information and examples of different emotion models
Shaver et al. - hierachical emotion model
Gui et al. - public dataset based on SINA city news.
},
	file = {Wang et al. - 2020 - A review of emotion sensing categorization models.pdf:/home/albert/Zotero/storage/C5Y56QRX/Wang et al. - 2020 - A review of emotion sensing categorization models.pdf:application/pdf},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26: 27th {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2013. {Proceedings} of a meeting held {December} 5-8, 2013, {Lake} {Tahoe}, {Nevada}, {United} {States}},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Gregory S. and Dean, Jeffrey},
	year = {2013},
	keywords = {word2vec},
	pages = {3111--3119},
	file = {arXiv\:1310.4546 PDF:/home/albert/Zotero/storage/695ESUSG/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf},
}

@inproceedings{pennington_glove:_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {https://www.aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2019-12-12},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	month = oct,
	year = {2014},
	keywords = {word embeddings},
	pages = {1532--1543},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:/home/albert/Zotero/storage/ZBQABJR6/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
}

@article{scharl_semantic_2017,
	title = {Semantic {Systems} and {Visual} {Tools} to {Support} {Environmental} {Communication}},
	volume = {11},
	copyright = {All rights reserved},
	doi = {10.1109/JSYST.2015.2466439},
	abstract = {Given the intense attention that environmental topics such as climate change attract in news and social media coverage, key questions are how different stakeholders perceive observable threats and policy options, how public media react to new scientific insights, and how journalists present climate science knowledge to the public. This paper investigates the potential of semantic technologies to address these questions. After summarizing methods to extract and disambiguate context information, we present visualization techniques to explore the lexical, geospatial and relational context of topics and entities referenced in these repositories. The examples stem from the Media Watch on Climate Change, the Climate Resilience Toolkit and the NOAA Media Watch - three applications that aggregate environmental resources from a wide range of online sources. These systems not only show the value of providing comprehensive information to the public, but also have helped to develop a novel communication success metric that goes beyond bipolar assessments of sentiment.},
	number = {2},
	journal = {IEEE Systems Journal},
	author = {Scharl, Arno and Herring, David and Rafelsberger, Walter and Hubmann-Haidvogel, Alexander and Kamolov, Ruslan and Fischl, Daniel and Föls, Michael and Weichselbraun, Albert},
	year = {2017},
	keywords = {annotation, climate science, context information, relation extraction, visual analytics, web intelligence},
	pages = {762--771},
	file = {Scharl et al. - 2017 - Semantic Systems and Visual Tools to Support Envir.pdf:/home/albert/Zotero/storage/3VIFSWB3/Scharl et al. - 2017 - Semantic Systems and Visual Tools to Support Envir.pdf:application/pdf},
}

@article{li_effect_2014,
	title = {The effect of news and public mood on stock movements},
	volume = {278},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025514003879},
	doi = {10.1016/j.ins.2014.03.096},
	abstract = {With technological advancements that cultivate vibrant creation, sharing, and collaboration among Web users, investors can rapidly obtain more valuable and timely information. Meanwhile, the adaption of user engagement in media effectively magnifies the information in the news. With such rapid information influx, investor decisions tend to be influenced by peer and public emotions. An effective methodology to quantitatively analyze the mechanism of information percolation and its degree of impact on stock markets has yet to be explored. In this article, we propose a quantitative media-aware trading strategy to investigate the media impact on stock markets. Our main findings are that (1) fundamental information of firm-specific news articles can enrich the knowledge of investors and affect their trading activities; (2) public sentiments cause emotional fluctuations in investors and intervene in their decision making; and (3) the media impact on firms varies according to firm characteristics and article content.},
	urldate = {2016-02-24},
	journal = {Information Sciences},
	author = {Li, Qing and Wang, TieJun and Li, Ping and Liu, Ling and Gong, Qixu and Chen, Yuanzhu},
	month = sep,
	year = {2014},
	keywords = {News, Sentiment Analysis, social media, Stock market, text mining},
	pages = {826--840},
	file = {Li et al. - 2014 - The effect of news and public mood on stock moveme.pdf:/home/albert/Zotero/storage/4AIV6JCP/Li et al. - 2014 - The effect of news and public mood on stock moveme.pdf:application/pdf},
}

@inproceedings{weichselbraun_extracting_2016,
	address = {Kauai, Hawaii},
	title = {Extracting {Opinion} {Targets} from {Environmental} {Web} {Coverage} and {Social} {Media} {Streams}},
	copyright = {All rights reserved},
	abstract = {Policy makers and environmental organizations have a keen interest in awareness building and the evolution of stakeholder opinions on environmental issues. Mere polarity detection, as provided by many existing methods, does not suffice to understand the emergence of collective awareness. Methods for extracting affective knowledge should be able to pinpoint opinion targets within a thread. Opinion target extraction provides a more accurate and fine-grained identification of opinions expressed in online media. This paper compares two different approaches for identifying potential opinion targets and applies them to comments from the YouTube video sharing platform. The first approach is based on statistical keyword analysis in conjunction with sentiment classification on the sentence level. The second approach uses dependency parsing to pinpoint the target of an opinionated term. A case study based on YouTube postings applies the developed methods and measures their ability to handle noisy input data from social media streams.},
	booktitle = {Proceedings of the 49th {Hawaii} {International} {Conference} on {System} {Sciences} ({HICSS}-49)},
	publisher = {IEEE Computer Society Press},
	author = {Weichselbraun, Albert and Scharl, Arno and Gindl, Stefan},
	month = jan,
	year = {2016},
	note = {Accepted 17 August 2015},
    doi = {10.1109/hicss.2016.133},
	keywords = {climate change, keyword analysis, Opinion mining, opinion target extraction, sentiment analysis},
}

@article{weichselbraun_adapting_2021,
	title = {Adapting {Data}-{Driven} {Research} to the {Fields} of {Social} {Sciences} and the {Humanities}},
	volume = {13},
	issn = {1999-5903},
	doi = {10.3390/fi13030059},
	abstract = {Recent developments in the fields of computer science such as advances in the areas of big data, knowledge extraction and deep learning have triggered the application of data-driven research methods to disciplines such as social sciences and humanities. This article presents a collaborative, interdisciplinary process for adapting data-driven research to research questions within other disciplines that considers the methodological background required to obtain a significant impact on the target discipline, guides the systematic collection and formalization of domain knowledge, the selection of appropriate data sources and methods for analyzing, visualizing and interpreting the results. 
Finally, we present a case study that applies the described process to the domain of communication science by creating approaches that aid domain experts in locating, tracking, analyzing and finally better understanding the dynamics of media criticism.  The study clearly demonstrates the potential of the presented method but also shows that data-driven research approaches require a tighter integration with the methodological framework of the target discipline to really provide a significant impact on the target discipline.},
	number = {3},
	journal = {Future Internet},
	author = {Weichselbraun, Albert and Kuntschik, Philipp and Fancolino, Vincenzo and Saner, Mirco and Wyss, Vinzenz},
	year = {2021},
	note = {Accepted 22 February 2021},
}

@article{reis_transformers_2021,
	title = {Transformers aftermath: current research and rising trends},
	volume = {64},
	issn = {0001-0782},
	shorttitle = {Transformers aftermath},
	url = {https://doi.org/10.1145/3430937},
	doi = {10.1145/3430937},
	abstract = {Attention, particularly self-attention, is a standard in current NLP literature, but to achieve meaningful models, attention is not enough.},
	number = {4},
	urldate = {2021-05-18},
	journal = {Communications of the ACM},
	author = {Reis, Eduardo Souza Dos and Costa, Cristiano André Da and Silveira, Diórgenes Eugênio Da and Bavaresco, Rodrigo Simon and Righi, Rodrigo Da Rosa and Barbosa, Jorge Luis Victória and Antunes, Rodolfo Stoffel and Gomes, Márcio Miguel and Federizzi, Gustavo},
	month = mar,
	year = {2021},
	pages = {154--163},
	annote = {Summary
This overview article discusses the transformer architecture, ground breaking models and open issues in the use of transformers for natural language understanding.
Background

most NLP research relies upon a sentence-to-sentence modelling:- input: x\_1, ... x\_n- output: y\_1, .. y\_n-1 - determine y\_n given intermediate representations z\_1, ... z\_n
solution: RNNs - predict y\_n given the distribution of previous inputs- problem:vanishing gradient problem; addressed by a forgetting mechanism- training time increases exponentially with the context vector
encoder-decoder architecture: - fed with input tokens x\_i- yields a hidden states h\_1, ... h\_n- the last hidden state is fed into a decoder which yields y\_1, ... y\_n based on h\_n and the decoders hidden state d\_1, ... d\_n
Transformers draw upon attention rather than recurrence.- recurrence is replaced by a stack of encoders and decoders- input x\_1, ... x\_n is projected into three vector spaces: (i) query (decoder), (ii) keys, and (iii) values (encoder)- queries: features relevant to the previous decoder- dot product yields the similarity between the input and the query keys- exponential softmax function increases the gap between relevant keys and less relevant ones.

Advantages of attention

multiple forms of attention identify relevant tokens
faster than recurrent layers, when the input sequence is shorter than the context vector

Types of attention

self-attention: correlates positions within the same sequence (e.g., sentence)
multihead attention: each attention head yields different weights (i.e., focuses on a different aspect of the problem) similar to an ensemble model.

Alternative approaches

CNNs: more parallelized
multi-step attention: decoder receives a matrix of attention weights from the previous decoder and computes its own

Unsupervised transformers

unsupervised pre-training based on large corpora
semi-supervised approaches are fine-tuned on task-specific data (e.g., BERT)
task agnostic: use a single model architecture across all tasks (e.g. by using a delimiter between input and target for summarization or question answering tasks)
GPT2: model size should allow for task solving tasks without the fine-tuning step.

Research directions

commonsense learning- RNNs outperform transformers when sentence lengths differ too much (!)- combine knowledge graphs or knowledge distillation with unsupervised pre-training- apply data augmentation and increase data diversity instead of data volume (e.g., by paraphrasing sentences for question answering tasks)
multitask learning

Enhancements

Transformer-XL model: map longer dependencies among the input tokens by caching context representations
RoBERTa optimizations for BERT-based models: use bigger training batches and remove the next sentence prediction (NSP) step
XLNet: learns dependencies independent of the token order (i.e., sequence of tokens)
ALBERT:remove the number of parameters by sharing parameters across layers (smaller models but computationally more complex)
GPT-3: large-scale model with 175 billion parameters; produces news articles that are hard to distinguish from human-written text.

Integrating domain knowledge

should help in mapping meaning rather than "statistical" predictions
two approaches:

knowledge graphs (leverage prior world knowledge that no longer needs to be stored in the model; reduces the attention search space; enables the ue of graph-based methods)
knowledge distillation (learn a compressed knowledge representation (student) from a huge (teacher model) while minimizing the performance loss- DistilBERT: 97 \% of model performance at 40\% of the size- Shallow models (shallow bidirectional LSTM achieves comparable results to ELMo with 1/100 of the parameters



Interesting: models that draw upon background knowledge (e.g., Amnervaz et al. 2018)},
	file = {Reis et al. - 2021 - Transformers aftermath current research and risin.pdf:/home/albert/Zotero/storage/YPRNSTBE/Reis et al. - 2021 - Transformers aftermath current research and risin.pdf:application/pdf},
}

@article{ding_jel_2021,
	title = {{JEL}: {Applying} {End}-to-{End} {Neural} {Entity} {Linking} in {JPMorgan} {Chase}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{JEL}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17796},
	language = {en},
	number = {17},
	urldate = {2021-06-03},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ding, Wanying and Chaudhri, Vinay K. and Chittar, Naren and Konakanchi, Krihshna},
	month = may,
	year = {2021},
	note = {Number: 17},
	keywords = {business intelligence, Deep \& Wide Learning, Named entity linking},
	pages = {15301--15308},
	annote = {Summary
Named entity linking in the business domain is a challenging task, since most state of the art methods rely upon comprehensive context information (e.g., from Wikipedia) that is not available for many business entities.
This paper, therefore, addresses an end-to-end neural entity linking model (JEL) that uses minimal context information and margin loss to generate entity embeddings and a Wide \& Deep Learning model that draws upon these embeddings for entity linking.
Method
The authors use (i) spaCy for entity recognition and (ii) their method for linking to identified entities.
Entit Embedding
triple loss model that selects

10 terms that are positive context examples for an entity, and
10 randomly selected negative examples (how representative are these examples; effectiveness?)

to compute contextualized entity embeddings Enitity Linking

Character matching: Wide Character Learning with subword information for matching entities (more effective than verbatim matching since it also considers typos)
Semantic matching: Use deep semantic embeddings to embed mentions based on the context into a vector. Compute the similarity between mention-vectors and entity-vectors based on Euclidean distance.

Application
similar to WISDOM - identify businesses in financial news and propagate the impact of the coverage (e.g., financial difficulties) along the supply chain.},
	file = {Snapshot:/home/albert/Zotero/storage/53XPV5G9/17796.html:text/html;Ding et al. - 2021 - JEL Applying End-to-End Neural Entity Linking in .pdf:/home/albert/Zotero/storage/4KWA6M6F/Ding et al. - 2021 - JEL Applying End-to-End Neural Entity Linking in .pdf:application/pdf},
}

@article{fu_spanner_2021,
	title = {{SpanNER}: {Named} {Entity} {Re}-/{Recognition} as {Span} {Prediction}},
	shorttitle = {{SpanNER}},
	url = {http://arxiv.org/abs/2106.00641},
	abstract = {Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction. Despite its preliminary effectiveness, the span prediction model's architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems' outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all code and datasets available: {\textbackslash}url\{https://github.com/neulab/spanner\}, as well as an online system demo: {\textbackslash}url\{http://spanner.sh\}. Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: {\textbackslash}url\{http://explainaboard.nlpedia.ai/leaderboard/task-ner/\}.},
	urldate = {2021-06-11},
	journal = {arXiv:2106.00641 [cs]},
	author = {Fu, Jinlan and Huang, Xuanjing and Liu, Pengfei},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.00641},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by ACL 2021 (Main track)},
    doi = {10.18653/v1/2021.acl-long.558},
	file = {arXiv.org Snapshot:/home/albert/Zotero/storage/4LF375ZK/2106.html:text/html;Fu et al_2021_SpanNER.pdf:/home/albert/Zotero/storage/3IEZ8VGV/Fu et al_2021_SpanNER.pdf:application/pdf},
}

@article{convertino_usefulness_2018,
	title = {The usefulness of listening social media for pharmacovigilance purposes: a systematic review},
	volume = {17},
	issn = {1744-764X},
	shorttitle = {The usefulness of listening social media for pharmacovigilance purposes},
	doi = {10.1080/14740338.2018.1531847},
	abstract = {INTRODUCTION: Social media mining could be a possible strategy to retrieve drug safety information. The mining of social media is a complex process under progressive evolution, falling into three broad categories: listening (safety data reporting), engaging (follow-up), and broadcasting (risk communication). This systematic review is aimed at evaluating the usefulness and quality of proto-signals by social media listening. Areas covered: In this systematic search, performed according to MOOSE and PRISMA statements, we selected studies, published in MEDLINE, EMBASE, and Google Scholar until 31 December 2017, that listened at least one social media to identify proto-adverse drug events and proto-signals. Expert opinion: The selected 38 studies identified serious and unexpected proto-adverse drug events characterized by poorer information quality as compared with spontaneous reporting databases. This feature allows rarely the evaluation of causal relationships. Proto-signals identified by social media listening had the potential of anticipating pre-specified known signals in only six studies. Moreover, the personal perception of patients reported in social media could be used to implement effective risk communication strategies. However, signal detection in social media cannot be currently recommended for routine pharmacovigilance, due to logistic and technical issues.},
	language = {eng},
	number = {11},
	journal = {Expert Opinion on Drug Safety},
	author = {Convertino, Irma and Ferraro, Sara and Blandizzi, Corrado and Tuccori, Marco},
	month = nov,
	year = {2018},
	pmid = {30285501},
	keywords = {Adverse Drug Reaction Reporting Systems, Data Mining, Databases, Factual, Drug-Related Side Effects and Adverse Reactions, Humans, pharmacovigilance, Pharmacovigilance, proto-signal, signal detection, Social media, Social Media},
	pages = {1081--1093},
	file = {Convertino et al_2018_The usefulness of listening social media for pharmacovigilance purposes.pdf:/home/albert/Zotero/storage/TKI5AH7M/Convertino et al_2018_The usefulness of listening social media for pharmacovigilance purposes.pdf:application/pdf},
}

@article{harris_distributional_1954,
	title = {Distributional {Structure}},
	volume = {10},
	issn = {0043-7956},
	url = {https://doi.org/10.1080/00437956.1954.11659520},
	doi = {10.1080/00437956.1954.11659520},
	number = {2-3},
	urldate = {2021-06-29},
	journal = {WORD},
	author = {Harris, Zellig S.},
	month = aug,
	year = {1954},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00437956.1954.11659520},
	pages = {146--162},
	file = {Harris_1954_Distributional Structure.pdf:/home/albert/Zotero/storage/59ZE8S3N/Harris_1954_Distributional Structure.pdf:application/pdf;Snapshot:/home/albert/Zotero/storage/HTSHI7XH/00437956.1954.html:text/html},
}

@misc{ia-commons,
    author = {Ilya Kreymer and Sebastian Nagel and Andy Jackson and Noah Levitt},
    title = {IIPC Web Archive Commons - Utility code for OpenWayBack and other projects.},
    year = {2021},
    publisher = {GitHub},
    journal = {GitHub repository},
    url = {https://github.com/commoncrawl/ia-web-commons},
    note = {Accessed: 2021-09-02}
}


@misc{html2text,
    author = {Aaron Swartz},
    title = {html2text - a Python script that converts a page of HTML into clean, easy-to-read plain ASCII text},
    year = {2021},
    publisher = {GitHub},
    journal = {GitHub repository},
    url = {https://github.com/Alir3z4/html2text},
    note = {Accessed: 2021-09-02}
}

@misc{justext,
    author = {Michal Belica},
    title = {jusText - Heuristic based boilerplate removal tool},
    year = {2021},
    publisher = {GitHub},
    journal = {GitHub repository},
    url = {https://github.com/miso-belica/jusText},
    note = {Accessed: 2021-09-02}
}

@misc{selenium,
    author = {Jason Huggins and Paul Gross and Jie Tina Wang},
    title = {jusText - Heuristic based boilerplate removal tool},
    year = {2021},
    publisher = {The Selenium Project},
    url = {https://www.selenium.dev},
    note = {Accessed: 2021-09-02}
}

@misc{lynx,
    author = {Thomas E. Dickey},
    title = {Lynx - The Text Web-Browser},
    year = {2021},
    publisher = {Lynx Home Page},
    url = {https://lynx.invisible-island.net},
    note = {Accessed: 2021-09-02}
}

@misc{doccano,
    author={
            Hiroki Nakayama and
            Takahiro Kubo and
            Junya Kamura and
            Yasufumi Taniguchi and
            Xu Liang},
    title={{doccano}: Text Annotation Tool for Human},
    year = {2018},
    publisher = {GitHub},
    journal = {GitHub repository},
    url = {https://github.com/doccano/doccano},
    note = {Accessed: 2021-09-02}
}

@misc{beautifulsoup,
    author = {Leonard Richardson},
    title = {Beautiful Soup - a library that makes it easy to scrape information from web pages},
    year = {2021},
    publisher = {Python Package Index},
    journal = {PyPI repository},
    url = {https://pypi.org/project/beautifulsoup4/},
    note = {Accessed: 2021-09-02}
}

@misc{lxml,
    author = {Stefan Behnel and
        Martijn Faassen and
            Ian Bicking and
            Holger Joukl and
            Simon Sapin and 
            Marc-Antoine Parent and
            Olivier Grisel and 
            Kasimier Buchcik and
            Florian Wagner and
            Emil Kroymann and
            Paul Everitt and 
            Victor Ng and
            Robert Kern and
            Andreas Pakulat and
            David Sankel and
            Marcin Kasperski and
            Sidnei da Silva and
            Pascal Oberndörfer},
    title = {lxml - Processing XML and HTML with Python},
    year = {2021},
    publisher = {lxml Project},
    url = {https://lxml.de/},
    note = {Accessed: 2021-09-02}
}

%% auhors are based on https://yarnpkg.com/package/cheerio
@misc{cheerio,
    author = {Matthew Mueller and Felix Böhm and Jugglin Mike and David Chambers},
    title = {Cheerio - Fast, flexible, and lean implementation of core jQuery designed specifically for the server.},
    year = {2021},
    publisher = {GitHub},
    journal = {GitHub repository},
    url = {https://github.com/cheeriojs/cheerio},
    note = {Accessed: 2021-09-02}
}

@misc{boilerpy3,
    author = {John Riebold},
    title = {BoilerPy3 - Python port of Boilerpipe library},
    year = {2021},
    publisher = {GitHub},
    journal = {GitHub repository},
    url = {https://github.com/jmriebold/BoilerPy3},
    note = {Accessed: 2021-09-02}
}

@inproceedings{weichselbraun_harvest_2020,
	title = {Harvest - {An} {Open} {Source} {Toolkit} for {Extracting} {Posts} and {Post} {Metadata} from {Web} {Forums}},
	doi = {10.1109/WIIAT50758.2020.00065},
	abstract = {Automatic extraction of forum posts and metadata is a crucial but challenging task since forums do not expose their content in a standardized structure. Content extraction methods, therefore, often need customizations such as adaptations to page templates and improvements of their extraction code before they can be deployed to new forums. Most of the current solutions are also built for the more general case of content extraction from web pages and lack key features important for understanding forum content such as the identification of author metadata and information on the thread structure.This paper, therefore, presents a method that determines the XPath of forum posts, eliminating incorrect mergers and splits of the extracted posts that were common in systems from the previous generation. Based on the individual posts further metadata such as authors, forum URL and structure are extracted. We also introduce Harvest, a new open source toolkit that implements the presented methods and create a gold standard extracted from 52 different Web forums for evaluating our approach. A comprehensive evaluation reveals that Harvest clearly outperforms competing systems.},
	booktitle = {2020 {IEEE}/{WIC}/{ACM} {International} {Joint} {Conference} on {Web} {Intelligence} and {Intelligent} {Agent} {Technology} ({WI}-{IAT})},
	author = {Weichselbraun, Albert and Brasoveanu, Adrian M. P. and Waldvogel, Roger and Odoni, Fabian},
	month = dec,
	year = {2020},
	keywords = {Data mining, Feature extraction, Uniform resource locators, Natural Language Processing, Information Extraction, Corporate acquisitions, Forum Extraction, Intelligent agents, Metadata, Web pages},
	pages = {438--444},
	file = {IEEE Xplore Abstract Record:/home/albert/Zotero/storage/ANQDHTKQ/9457756.html:text/html;Weichselbraun et al_2020_Harvest - An Open Source Toolkit for Extracting Posts and Post Metadata from.pdf:/home/albert/Zotero/storage/9NQIW9XN/Weichselbraun et al_2020_Harvest - An Open Source Toolkit for Extracting Posts and Post Metadata from.pdf:application/pdf},
}

@inproceedings{peters_content_2013,
	title = {Content extraction using diverse feature sets},
	isbn = {978-1-4503-2038-2},
	url = {http://dl.acm.org/citation.cfm?id=2487788.2487828},
	doi = {10.1145/2487788.2487828},
	urldate = {2019-12-23},
	publisher = {ACM},
	author = {Peters, Matthew E. and Lecocq, Dan},
	month = may,
	year = {2013},
	keywords = {dragnet},
	pages = {89--90},
	file = {Snapshot:/home/albert/Zotero/storage/JZGCDEZ4/citation.html:text/html;Full Text PDF:/home/albert/Zotero/storage/6JI2BK3X/Peters and Lecocq - 2013 - Content extraction using diverse feature sets.pdf:application/pdf},
}


@inproceedings{lang_textsweeper_2012,
	address = {Vienna, Austria},
	title = {{TextSweeper} - {A} {System} for {Content} {Extraction} and {Overview} {Page} {Detection}},
	copyright = {All rights reserved},
	abstract = {Web pages not only contain main content, but also other elements such as navigation panels, advertisements and links to related documents. Furthermore, overview pages (summarization pages and entry points) duplicate and aggregate parts of articles and thereby create redundancies. The noise elements in Web pages as well as overview pages affect the performance of downstream processes such as Web-based Information Retrieval. Context Extraction's task is identifying and extracting the main content from a Web page. In this research-in-progress paper we present an approach which not only identifies and extracts the main content, but also detects overview pages and thereby allows skipping them. The content extraction part of the system is an extension of existing Text-to-Tag ratio methods, overview page detection is accomplished with the net text length heuristic. Preliminary results and ad-hoc evaluation indicate a promising system performance. A formal evaluation and comparison to other state-of-the-art approaches is part of future work.},
	booktitle = {International {Conference} on {Information} {Resources} {Management} ({Conf}-{IRM})},
	publisher = {AIS},
	author = {Lang, Heinz-Peter and Wohlgenannt, Gerhard and Weichselbraun, Albert},
	year = {2012},
	keywords = {natural language processing, content extraction, contextualized information spaces, overview pages, text filtering, Web-based information retrieval},
	file = {lang2012-textSweeper.pdf:/home/albert/Zotero/storage/BKDTK3SJ/lang2012-textSweeper.pdf:application/pdf},
    url = {https://aisel.aisnet.org/confirm2012/17/},
}


@article{caswell_quality_2021,
	title = {Quality at a {Glance}: {An} {Audit} of {Web}-{Crawled} {Multilingual} {Datasets}},
	shorttitle = {Quality at a {Glance}},
	url = {http://arxiv.org/abs/2103.12028},
	abstract = {With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, web-mined text datasets covering hundreds of languages. However, to date there has been no systematic analysis of the quality of these publicly available datasets, or whether the datasets actually contain content in the languages they claim to represent. In this work, we manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4), and audit the correctness of language codes in a sixth (JW300). We find that lower-resource corpora have systematic issues: at least 15 corpora are completely erroneous, and a significant fraction contains less than 50\% sentences of acceptable quality. Similarly, we find 82 corpora that are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-speakers of the languages in question, and supplement the human judgements with automatic analyses. Inspired by our analysis, we recommend techniques to evaluate and improve multilingual corpora and discuss the risks that come with low-quality data releases.},
	urldate = {2021-09-03},
	author = {Caswell, Isaac and Kreutzer, Julia and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone and Setyawan, Monang and Sarin, Supheakmungkol and Samb, Sokhar and Sagot, Benoît and Rivera, Clara and Rios, Annette and Papadimitriou, Isabel and Osei, Salomey and Suárez, Pedro Javier Ortiz and Orife, Iroro and Ogueji, Kelechi and Niyongabo, Rubungo Andre and Nguyen, Toan Q. and Müller, Mathias and Müller, André and Muhammad, Shamsuddeen Hassan and Muhammad, Nanda and Mnyakeni, Ayanda and Mirzakhalov, Jamshidbek and Matangira, Tapiwanashe and Leong, Colin and Lawson, Nze and Kudugunta, Sneha and Jernite, Yacine and Jenny, Mathias and Firat, Orhan and Dossou, Bonaventure F. P. and Dlamini, Sakhile and de Silva, Nisansa and Ballı, Sakine Çabuk and Biderman, Stella and Battisti, Alessia and Baruwa, Ahmed and Bapna, Ankur and Baljekar, Pallavi and Azime, Israel Abebe and Awokoya, Ayodele and Ataman, Duygu and Ahia, Orevaoghene and Ahia, Oghenefego and Agrawal, Sweta and Adeyemi, Mofetoluwa},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.12028},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Caswell et al_2021_Quality at a Glance.pdf:/home/albert/Zotero/storage/K3HBQMTH/Caswell et al_2021_Quality at a Glance.pdf:application/pdf;arXiv.org Snapshot:/home/albert/Zotero/storage/XT4AS68K/2103.html:text/html},
}

@inproceedings{el-kishky_ccaligned_2020,
	address = {Online},
	title = {{CCAligned}: {A} {Massive} {Collection} of {Cross}-{Lingual} {Web}-{Document} {Pairs}},
	shorttitle = {{CCAligned}},
	url = {https://aclanthology.org/2020.emnlp-main.480},
	doi = {10.18653/v1/2020.emnlp-main.480},
	abstract = {Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5\% across different language pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In addition to curating this massive dataset, we introduce baseline methods that leverage cross-lingual representations to identify aligned documents based on their textual content. Finally, we demonstrate the value of this parallel documents dataset through a downstream task of mining parallel sentences and measuring the quality of machine translations from models trained on this mined data. Our objective in releasing this dataset is to foster new research in cross-lingual NLP across a variety of low, medium, and high-resource languages.},
	urldate = {2021-09-03},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {El-Kishky, Ahmed and Chaudhary, Vishrav and Guzmán, Francisco and Koehn, Philipp},
	month = nov,
	year = {2020},
	pages = {5960--5969},
	file = {El-Kishky et al_2020_CCAligned.pdf:/home/albert/Zotero/storage/ZE439VUV/El-Kishky et al_2020_CCAligned.pdf:application/pdf},
}

@inproceedings{xue_mt5_2021,
	address = {Online},
	title = {{mT5}: {A} {Massively} {Multilingual} {Pre}-trained {Text}-to-{Text} {Transformer}},
	shorttitle = {{mT5}},
	url = {https://aclanthology.org/2021.naacl-main.41},
	doi = {10.18653/v1/2021.naacl-main.41},
	abstract = {The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.},
	urldate = {2021-09-03},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
	month = jun,
	year = {2021},
	pages = {483--498},
	file = {Xue et al_2021_mT5.pdf:/home/albert/Zotero/storage/X32LQJ5V/Xue et al_2021_mT5.pdf:application/pdf},
}

@inproceedings{suarez_asynchronous_2019,
	title = {Asynchronous {Pipeline} for {Processing} {Huge} {Corpora} on {Medium} to {Low} {Resource} {Infrastructures}},
	url = {https://hal.inria.fr/hal-02148693},
	doi = {10.14618/IDS-PUB-9021},
	abstract = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
	language = {en},
	urldate = {2021-09-03},
	publisher = {Leibniz-Institut für Deutsche Sprache},
	author = {Suárez, Pedro Javier Ortiz and Sagot, Benoît and Romary, Laurent},
	month = jul,
	year = {2019},
	file = {Suárez et al_2019_Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource.pdf:/home/albert/Zotero/storage/UZBC8FDK/Suárez et al_2019_Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource.pdf:application/pdf;Snapshot:/home/albert/Zotero/storage/FRLXKUEI/hal-02148693.html:text/html},
}


